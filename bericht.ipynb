{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bericht: Philipp Hainhofer\n",
    "Reiseberichte und Sammlungsbeschreibungen 1594-1636\n",
    "\n",
    "Unser Projekt basiert auf dem Quellenkorpus der digitalen Edition von Philipp Hainhofers Reise- und Sammlungsbeschreibungen (1594-1636), die systematisch erschlossen und für computergestützte Analyse zugänglich gemacht wurden. Das Projekt wird von der Deutschen Forschungsgemeinschaft gefördert und ist eine Kooperation zwischen der Herzog August Bibliothek und der Universität Trier.\n",
    "Im Zentrum stehen Hainhofers Reisen an europäische Höfe, seine diplomatischen Missionen und seine Rolle als Kunstvermittler. Die Texte bieten Einblicke in politische, kulturelle und materielle Austauschprozesse des 17. Jahrhunderts.\n",
    "Diese digitale Edition erschliesst diese Quelle erstmals systematisch und macht sie so für die computergestützte Analyse nutzbar.\n",
    "\n",
    "Philipp Hainhofer war eine zentrale Figur des kulturellen Austauschs im frühneuzeitlichen Europa. Als „cultural broker“ (Einführungstext der Edition und Datensammlung zur Kunst- und Kulturgeschichte der ersten Hälfte des 17. Jahrhunderts, hrsg. und eingeleitet von Michael Wenzel, Wolfenbüttel: Herzog August Bibliothek, 2020–2025)  bewegte er sich zwischen Politik, Diplomatie, Kunsthandel und höfischer Kultur. Seine Reiserelationen bilden den Kern seines schriftlichen Nachlasses und sind eine wertvolle Quelle für Kunstgeschichte, Geschichtswissenschaften und angrenzende Disziplinen.\n",
    "Ziel des Editionsprojekts ist die digitale Erschliessung dieser Reiseberichte - durch Faksimiles, Volltexte, kritische Apparate und Sachkommentare. Die Reiseberichte von 1603 bis 1636 sind bereits digital ediert und weitere folgen bis zum geplanten Projektabschluss im Jahr 2029. Neben der quellenkritischen Aufbereitung verfolgt das Projekt zwei wissenschaftliche Leitziele: die Untersuchung kultureller Vermittlung in einem konfliktreichen Europa und einen interdisziplinären Ansatz, der historische und kunsthistorische Perspektiven integriert. (Kommentierung zur Edition https://www.hab.de/kommentierte-digitale-edition-der-reise-und-sammlungsbeschreibungen-philipp-hainhofers-1578-1647/ letzter Zugriff 5.7.25)\n",
    "\n",
    "Wer war Philipp Hainhofer?\n",
    "Philipp Hainhofer (1578-1647) war eine bedeutende Vermittlerfigur im Europa des 17. Jahrhunderts. Er wurde in Augsburg geboren, studierte unter anderem in Italien und verfügte über umfangreiche Sprachkenntnisse, die ihm seine spätere, internationale Tätigkeit ermöglichten.\n",
    "Er war im Auftrag verschiedener Fürsten, Diplomaten und Städten tätig - unabhängig von deren Konfession - und pflegte ein weit verzweigtes Netzwerk in ganz Europa. Als sogenannter „cultural broker“ überschritt er politische, religiöse und kulturelle Grenzen und vermittelte nicht nur Kunstobjekte, Bücher und Möbel, sondern auch Informationen, Kontakte und Wissen.\n",
    "Die Hauptphase seiner Tätigkeit lag zwischen 1610 und 1620. Aus diesem Zeitraum stammen auch die meisten der Reiseberichte, die wir für unser Projekt ausgewertet haben.\n",
    "\n",
    "Der verwendete Quellenkorpus ist aus mehreren Gründen gut für unser Projekt geeignet:\n",
    "Die Texte liegen bereits strukturiert vor, sind digital durchsuchbar und für maschinelle Auswertungen aufbereitet.\n",
    "Inhaltlich bieten sie detaillierte Einblicke in die politischen, sozialen und kulturellen Netzwerke des 17. Jahrhunderts. Hainhofer dokumentiert seine Begegnungen, den Austausch von Objekten, die Beschreibung von Orten sowie den jeweiligen politischen Kontext.\n",
    "Zudem zeichnet sich der Korpus durch eine grosse zeitliche und räumliche Spannweite aus. Er umfasst über vier Jahrzehnte Reisetätigkeit und zahlreiche europäische Regionen. Diese Eigenschaften machen ihn besonders geeignet für digitale Methoden wie Netzwerkanalyse oder geobasierte Visualisierungen. Dies ist eine ideale Grundlage um mit digitalen Methoden Netzwerke, Bewegungen und Austauschprozesse sichtbar zu machen.\n",
    "\n",
    "Die zentrale Forschungsfrage unseres Projekts lautet: Wie verändern sich Hainhofers Netzwerke im Zeitverlauf, und welche Rolle spielen dabei Konfession und soziale Stellung der besuchten Personen?\n",
    "Ziel ist es, zu untersuchen, ob und wie sich Zusammensetzung seines Kontaktnetzwerks über die Jahre verschieben. Filter, welche uns geeignet erschienen waren: Zeiträume, geografische Regionen und konfessionelle Zugehörigkeit. \n",
    "\n",
    "Im ersten Schritt haben wir das Tool Voyant genutzt, um häufig vorkommende Begriffe im Korpus zu identifizieren und so erste inhaltliche Schwerpunkte und mögliche Ansatzpunkte für die vertiefende Analyse zu erkennen.\n",
    "\n",
    "Im weiteren Verlauf des Projekts haben wir uns dazu entschieden, mit den Personen- und Objekteregistern  der digitalen Edition zu arbeiten. Dabei zeigte sich, dass die tabellarische Erfassung am besten geeignet ist, um Personen, Orte und Objekte systematisch zu dokumentieren und eine Auswertung zu ermöglichen. Unser Vorgehen sieht daher vor, zunächst eine einzelne Reise vollständig zu erschliessen und darauf aufbauend das Projekt schrittweise zu erweitern.\n",
    "    \n",
    "Vorgehen\n",
    "Ausgangspunkt ist die Arbeit mit den vorhandenen Registern, aus denen wir relevante Entitäten (insbesondere Personen und Orte) im XML-Format extrahieren. Im nächsten Schritt erfolgt ein Entity Matching mit externen Referenzdaten aus dem Bereich der Linked Open Data (LOD), um die Einträge eindeutig zuzuordnen und anzureichern.\n",
    "Für die geobasierte Analyse werden die geografischen Koordinaten der erfassten Orte über den Dienst Geonames abgerufen. Darauf aufbauend erfolgt die Visualisierung der besuchten Orte sowie die Visualisierung der in den Texten beschriebenen Netzwerke.\n",
    "\n",
    "Weiterführende Forschung\n",
    "Potenzielle weiterführende Forschungsideen ergeben sich insbesondere aus der Skalierbarkeit von unserem Ansatz. Eine Möglichkeit besteht darin, die Analyse auf weitere Reisen Philipp Hainhofers auszuweiten. Die systematische Erfassung zusätzlicher Reiseberichte würde es erlauben, Veränderungen in Struktur und Zusammensetzung seines Netzwerks über längere Zeiträume hinweg zu analysieren: zum Beispiel im Zusammenhang mit politischen Umbrüchen wie dem Dreissigjährigen Krieg.\n",
    "\n",
    "Darüber hinaus bietet der Einsatz von Entity Matching mit Linked Open Data grosses Potenzial für die kontextuelle Anreicherung historischer Daten. Durch die Anbindung an etablierte Datenquellen könnten biografische, geografische und soziale Informationen ergänzt und vertieft werden.\n",
    "\n",
    "Eine weitere Ausbaumöglichkeit liegt in der Entwicklung dynamischer, zeitbasierter Visualisierungen. Interaktive Karten mit Zeitachsen könnten weitere Muster in Hainhofers Reisetätigkeit sichtbar machen: beispielsweise hinsichtlich Reisedauer, wiederholt besuchter Orte oder der Dichte und Stabilität seiner Netzwerke über die Zeit hinweg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
# -------- Personennamen: Normdatenverknüpfung & Metadatenanreicherung --------

import re
import sys
import pandas as pd
from lxml import etree as ET
from openpyxl import load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, PatternFill

# Konstanten
lod_file = 'hainhofer-lod.xml' # LOD-Mapping-Datei
reg_file = 'register.xhtml' # Personenregister-Datei
base_url = 'https://hainhofer.hab.de/register/personen/' # Basis-URL für PSN-IDs


# Namespaces für die LOD-Datei
namespaces_lod = {
    'rdf': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#',
    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',
    'schema': 'http://schema.org/',
    'gndo': 'https://d-nb.info/standards/elementset/gnd#',
    'ecrm': 'http://erlangen-crm.org/current/',
    'psn': 'https://hainhofer.hab.de/register/personen/',
    'wdt': 'http://www.wikidata.org/prop/direct/',
}
# Namespaces für die Berichte
namespaces_report = {
    'tei': 'http://www.tei-c.org/ns/1.0'
}

# Wikidata-Konfessionen
wikidata_faith_map = {
    'Q1841': 'katholisch',
    'Q23540': 'protestantisch',
    'Q106039': 'christlich allgemein'
}

# XPath-Ausdrücke zur Lokalisierung spezifischer Metadaten im Personenregister
xpath_entries = '//div[@id="psn"]/div[contains(@class, "entry")]' # Personeneintrag
xpath_name = './/h1[@class="prefname"]/text()' # Name
xpath_lifedata = './/div[@class="birthdeath"]/p/text()' # Lebensdaten (Geburts- und Todesjahr)
xpath_category = './/div[@class="categorycontainer"]//li' # Stand/Tätigkeit
xpath_faith = './/div[@class="faithcontainer"]//li/text()' # Konfession

# Funktionen
def parse_reg_metadata(reg_file):
    """
    Parst die Register-XHTML-Datei und extrahiert Metadaten zu Personen.

    Args:
        reg_file_path (str): Pfad zur Datei 'register.xhtml'.

    Returns:
        pd.DataFrame: DataFrame mit extrahierten Personen-Metadaten.
    """
    try:
        tree_xhtml = ET.parse(reg_file, ET.HTMLParser())
    except FileNotFoundError:
        print(f"Die Datei '{reg_file}' wurde nicht gefunden. Bitte stellen Sie sicher, dass der Pfad korrekt ist.")
        sys.exit(1)
    except ET.XMLSyntaxError as e:
        print(f"Fehler beim Parsen von '{reg_file}': {e}")
        sys.exit(1)
    except Exception as e:
        print(f"Fehler beim Laden von '{reg_file}': {e}")
        sys.exit(1)

def extract_text_from_elem(elem):
    """
    Extrahiert den gesamten Text aus einem lxml-Element und normalisiert Leerschläge.
    Entfernt mehrfache Leerschläge durch einfache und trimmt führende/abschliessende Leerzeichen.

    Args:
        elem (lxml.etree._Element): Das lxml-Element, aus dem der Text extrahiert werden soll.

    Returns:
        str: Der bereinigte und normalisierte Text des Elements.
    """
    return re.sub(r'\s+', ' ', ''.join(elem.itertext())).strip()

def extract_xpath_text(entry, xpath_expr):
    """
    Extrahiert den ersten passenden Text eines XPath-Ausdrucks aus einem lxml-Element und bereinigt ihn.

    Args:
        entry (lxml.etree._Element): Ein lxml-Element (z.B. ein Personeneintrag).
        xpath_expr (str): Ein XPath-Ausdruck, der den gewünschten Text identifiziert.

    Returns:
        str: Der bereinigte Text oder ein leerer String, wenn kein Ergebnis gefunden wird.
    """
    result = entry.xpath(xpath_expr)
    return result[0].strip() if result else None

def extract_list_text(entry, xpath_expr):
    """
    Extrahiert Textinhalte aus einer Liste von XML-Elementen, bereinigt sie
    und gibt sie als semikolon-getrennten String zurück. Nützlich für Listen
    von Kategorien oder Konfessionen.

    Args:
        entry (lxml.etree._Element): Das lxml-Element.
        xpath_expr (str): Der XPath-Ausdruck, der die Liste der Elemente identifiziert.

    Returns:
        str: Ein semikolon-getrennter String der extrahierten Texte.
    """
    items = entry.xpath(xpath_expr)
    list_texts = []
    for item in items:
        if isinstance(item, str): # Für XPath-Ausdrücke, die direkt Text zurückgeben
            text = item.strip()
        else: # Für XPath-Ausdrücke, die Elemente zurückgeben, deren Text extrahiert werden muss
            text = ''.join(item.itertext()).strip()
        text = re.sub(r'\s+', ' ', text) # Normalisiert Leerzeichen innerhalb des Textes
        if text:
            list_texts.append(text)
    return '; '.join(list_texts)

def extract_lifedata(text):
    """
    Extrahiert das Geburts- und Todesjahr aus einem Textstring.
    Behandelt sowohl einzelne Jahre als auch Jahresspannen
    (z.B. "1573", "260 v. Chr.", "zwischen ca. 980 und 986", "zwischen 290 v. Chr. und 399 n. Chr.").

    Args:
        text (str): Ein Textstring mit Lebensdaten.

    Returns:
        tuple: Ein Tupel bestehend aus zwei Angaben: (Geburtsjahr, Todesjahr).
               (None, None) wird ausgegeben, wenn kein passendes Muster gefunden wird.
    """
    return None, None

def extract_birth_date(lifedata):
    """
    Extrahiert das Geburtsjahr oder den entsprechenden Zeitraum aus einem Lebensdaten-String.
    Sucht nach dem Teil des Strings, der nach einem Sternzeichen ('*') beginnt und vor einem Kreuz ('✝') endet.

    Args:
        lifedata (str): Ein String mit den Lebensdaten der Person (z.B. "*1573 ✝1620").

    Returns:
        death_date_reg (str): Ein String mit dem Geburtsjahr oder dem entsprechenden Zeitraum.
    """

    if "*" in lifedata:
        text = lifedata[lifedata.find("*")+1:] # Nimmt den Text nach dem Sternzeichen
        text = text.split("✝")[0] # Nimmt den Text vor dem Kreuz
        return extract_lifedata(text)
    return ''

def extract_death_date(lifedata):
    """
    Extrahiert das Todesjahr oder den entsprechenden Zeitraum aus einem Lebensdaten-String.
    Sucht nach dem Teil des Strings, der nach einem Kreuz ('✝') beginnt.

    Args:
        lifedata (str): Ein String mit den Lebensdaten der Person (z.B. "*1573 ✝1620").

    Returns:
        death_date_reg (str): Ein String mit dem Todesjahr oder dem entsprechenden Zeitraum.
    """
    if "✝" in lifedata:
        text = lifedata[lifedata.find("✝")+1:]
        return extract_lifedata(text)
    return ''

def clean_cell(value):
    """
    Bereinigt den Wert einer Zelle für die Ausgabe in Tabellen (z.B. Excel).
    Konvertiert Listen von Werten in einen semikolon-getrennten String,
    entfernt führende/abschliessende Leerzeichen von Strings und behandelt NaN-Werte.

    Args:
        value (any): Ein zu bereinigender Wert.

    Returns:
        str: Der bereinigte Wert als String.
    """    
    if isinstance(value, list):
        return '; '.join([v.strip() if isinstance(v, str) else str(v) for v in value])
    elif isinstance(value, str):
        return value.strip()
    else:
        return '' if pd.isna(value) else str(value).strip()
    
def prove_metadata(row):
    """
    Entscheidet, welche Metadatenquelle verwendet werden soll:
    
    Regeln:
    - Wenn beide Quellen identisch sind, werden Registerdaten verwendet.
    - Wenn Registerdaten vorhanden sind, sich jedoch von der LOD-Version unterscheiden, werden LOD-Daten genommen.
    - Wenn LOD-Daten fehlen, werden Register-Daten bevorzugt.
    - Wenn beide Quellen leer sind, wird ein leerer String zurückgegeben.

    Args:
        row (pd.Series): Eine Zeile des DataFrames.

    Returns:
        Der ausgewählte Lebensdaten-String oder ein leerer String, wenn beide Quellen leer sind.
    """

    lod = row['Lebensdaten_LOD']
    reg = row['Lebensdaten_Register']

    if lod and reg:
        return reg if lod == reg else lod  # Bei Übereinstimmung: Register, sonst: LOD
    if not lod:
        return reg  # Nur Register vorhanden (auch wenn reg None ist): Register
    return lod  # Nur LOD vorhanden: LOD

def adjust_metadata(row):
    """
    Gibt ein dict zurück mit den zu aktualisierenden Metadaten:
    - Lebensdaten (LOD oder Register)
    - Konfession (LOD, wenn LOD-Lebensdaten verwendet werden, sonst Register)
    """
    if row['Lebensdaten_LOD'] and row['Lebensdaten_LOD'] != row['Lebensdaten_Register']:
        # LOD-Lebensdaten unterscheiden sich und werden bevorzugt
        return pd.Series({
            'Lebensdaten': row['Lebensdaten_LOD'],
            'Konfession': row.get('Konfession_LOD', None)
        })
    else:
        # Kein Unterschied oder LOD leer -> Registerdaten verwenden
        return pd.Series({
            'Lebensdaten': row['Lebensdaten_Register'],
            'Konfession': row.get('Konfession_Register')  if 'Konfession_Register' in row else None
        })

# --- Schritt 1: LOD-Mapping-Datei laden & PSN-IDs mit Personennamen verknüpfen ---
# Dieser Schritt liest die LOD-Datei ein und erstellt ein Mapping-Dictionary (psn_to_name),
# das PSN-IDs den standardisierten Personennamen zuordnet und Lebensdaten erfasst.

try:
    tree_lod = ET.parse(lod_file)
    root_lod = tree_lod.getroot()
except FileNotFoundError:
    print(f"Die Datei '{lod_file}' wurde nicht gefunden.")
    sys.exit(1)
except ET.XMLSyntaxError as e:
    print(f"Fehler beim Parsen der Datei '{lod_file}':\n{e}")
    sys.exit(1)
except Exception as e:
    print(f"Fehler beim Laden der Datei '{lod_file}':\n{e}")
    sys.exit(1)

# Liste zur Speicherung von LOD-Personendaten
lod_metadata = []

# Dictionary für die PSN-IDs der Personen
psn_name = {} # {PSN-ID: Personenname}

# Dictionary für Lebensdaten (Geburts- und Todesjahr) der Personen
psn_lifedata = {} # {PSN-ID: (Geburtsdatum, Todesdatum)}

# Basis-URL für Einträge im Personenregister (Personen-URL = Basis-URL + PSN-ID)
# Diese URL wird verwendet, um die PSN-ID aus der URL zu extrahieren
base_url = 'https://hainhofer.hab.de/register/personen/'

# Durchsucht die LOD-Datei nach Personeneinträgen
for entry_lod in root_lod.findall('.//rdf:Description', namespaces_lod):
    # Ungekürzte PSN-ID
    person_url_lod = entry_lod.find('schema:mainEntityOfPage', namespaces_lod)
    # Personenname
    person_label_lod = entry_lod.find('rdfs:label', namespaces_lod)

    # Überspringt Einträge, die keine gültige URL haben
    # oder deren URL nicht mit der Basis-URL des Personenregisters beginnt
    if person_label_lod is None or person_url_lod is None or not person_url_lod.text.startswith(base_url):
        continue

    # Extrahiert die PSN-ID aus der Personen-URL
    psn_id = person_url_lod.text[len(base_url):].strip()
    person_name = person_label_lod.text.strip()

    # Speichert die Zuordnung {PSN-ID: Personenname} im Dictionary
    psn_name[psn_id] = person_name

    # Extrahiert die Geburts- und Todesdaten aus den gndo-Tags
    birth_date = entry_lod.find('gndo:dateOfBirth', namespaces_lod)
    death_date = entry_lod.find('gndo:dateOfDeath', namespaces_lod)
    birth_date_lod = birth_date.text.strip() if birth_date is not None else None
    death_date_lod = death_date.text.strip() if death_date is not None else None

    # Tupel für Geburts- und Todesdatum: Ohne Konvertierung in numerische Werte, da die Lebensdaten in der LOD-Datei als Strings vorliegen.
    lifedata_lod = (birth_date_lod, death_date_lod)

    # Speichert die Zuordnung {PSN-ID: Lebensdaten_LOD} im Dictionary
    # Dies ermöglicht die spätere Verknüpfung der Lebensdaten mit den Personennennungen im Bericht
    psn_lifedata[psn_id] = lifedata_lod

    # Extrahiert die Konfession einer Person
    faith_lod_element = entry_lod.find('wdt:P140', namespaces_lod)
    if faith_lod_element is not None:
        wikidata_resource = faith_lod_element.attrib.get('w3:resource')
    # Extrahiert die Wikipedia-Ressource, falls vorhanden
    if wikidata_resource:
        qid = wikidata_resource.split('/')[-1]
    else:
        qid = None
    
    # Konvertiert die Wikidata QID in eine lesbare Konfession
    if faith_lod_element == 'Q1841':
        faith_lod = 'katholisch'
    elif faith_lod_element == 'Q23540':
        faith_lod = 'protestantisch'
    elif faith_lod_element == 'Q106039':
        faith_lod = 'christlich allgemein'

    # Fügt die Personendaten aus der LOD-Datei der Liste hinzu
    lod_metadata.append({
    'PSN-ID': psn_id,
    'Name': person_name,
    'Lebensdaten_LOD': lifedata_lod
    'Konfession_LOD': faith_lod if faith_lod else None,
    })

# Erstellt ein DataFrame aus den LOD-Personendaten
df_lod = pd.DataFrame(lod_metadata)

print(f"LOD-Mapping geladen. Es befinden sich {len(psn_name)} Personen im Mapping.")
# print(f"LOD-Lebensdaten für {len(df_lod)} Personen erfasst.")


# --- Schritt 2: XML-Berichtsdatei laden & Personennungen extrahieren ---
# Dieser Schritt fordert den Benutzer zur Eingabe eines Berichtsnamens auf (z.B. "München 1603"),
# lädt die entsprechende XML-Datei und extrahiert alle im Bericht als presenten Personen.

user_input = input('Geben Sie das Reiseziel und das Jahr des gesuchten Berichts an (z.B. München 1603): ')
report_name = user_input + '.xml'

try:
    tree_report = ET.parse(report_name)
    root_report = tree_report.getroot()
except FileNotFoundError:
    print(f"Die Datei '{report_name}' wurde nicht gefunden.")
    sys.exit(1)
except ET.XMLSyntaxError as e:
    print(f"Fehler beim Parsen der XML-Datei '{report_name}':\n{e}")
    sys.exit(1)
except Exception as e:
    print(f"Ein unerwarteter Fehler ist aufgetreten:\n{e}")
    sys.exit(1)

# Liste zur Speicherung aller Personennennungen und der damit entsprechenden PSN-IDs
all_mentions_report = []

# Sammelt Nennungen von Personen, die in der XML-Datei als 'present' markiert sind (d.h. unmittelbar in die Handlung involviert)
persons = root_report.findall('.//tei:rs[@type="person"][@role="present"]', namespaces_report)

for elem in persons:
    parent = elem.getparent()
    skip = False
    # Prüft auf editorische Notizen oder Inhalte von 'fremder Hand'
    while parent is not None:
        # Schliesst editorische Notizen aus
        if parent.tag == 'note' and parent.attrib.get('resp') == '#editor':
            skip = True
            break
        # Schliesst Inhalte von 'fremder Hand' aus
        if (parent.tag == 'p' or parent.tag == 'div') and parent.attrib.get('hand') == '#fremde_hand':
            skip = True
            break
        # Geht zum nächsthöheren Elternelement
        parent = parent.getparent()
    if skip:
        continue

    ref = elem.get('ref', None)
    # Stellt sicher, dass das 'ref'-Attribut existiert und mit 'psn:' beginnt
    # Dies ist notwendig, um die PSN-ID korrekt zu extrahieren
    if not ref or not ref.startswith('psn:'):
        continue
    # Extrahiert die PSN-ID aus dem 'ref'-Attribut
    psn = ref[4:] # Entfernt das Präfix 'psn:'
    # Extrahiert den reinen Text der Personennennung
    person_mention = extract_text_from_elem(elem)

    # Fügt die Verknüpfung {Personennennung: PSN-ID} zum Dictionary hinzu
    all_mentions_report.append({
    'Nennung': person_mention,
    'PSN-ID': psn
    })

print(f" Es wurden {len(all_mentions_report)} Personennennungen im Bericht '{report_name}' gefunden.")


# --- Schritt 3: Erstellung eines Dataframe mit den gemappten Personennamen ---
# In diesem Schritt werden die extrahierten Personennennungen (aus Schritt 2)
# mit den Personennamen aus dem psn_to_name Dictionary (aus Schritt 1) verknüpft.
# Das Ergebnis ist ein DataFrame mit der spezifischen Nennung und dem zugehörigen Personennamen.
# Die PSN-ID wird im Dataframe beibehalten, um die eindeutige Zuweisung der Lebensdaten zu ermöglichen.

# Liste zur Speicherung von Dictionaries mit der Verknüpfung
# {Nennung: Personenname} über die Verwendung der PSN-ID
mapped_persons = []

# Iteriert durch die Personennennungen
for person_mention, psn in all_mentions_report:
    # Holt den Personennamen aus dem zuvor geladenen psn_to_name Dictionary
    person_name = psn_name.get(psn)
    if person_name:
        # Fügt die Nennung und den Personennamen zur Liste hinzu
        mapped_persons.append({
            'Nennung': person_mention,
            'PSN-ID': psn,
            'Name': person_name
        })

# Erstellen des DataFrames für die Personennennungen im Bericht
df_report_mentions = pd.DataFrame(all_mentions_report)

# Führt die gemappten Personennennungen mit den PSN-IDs zusammen und fügt die PSN-ID hinzu
df_report_persons = pd.merge(
    df_report_mentions,
    df_lod,
    left_on='psn',
    right_on='PSN_ID',
    how='left'
).drop(columns=['PSN_ID'])

# Erstellt einen Pandas DataFrame aus den gemappten Personen
df_mapping = pd.DataFrame(mapped_persons)


# --- Schritt 4: XHTML-Register parsen & Metadaten erfassen ---
# Dieser Schritt liest das Personenregister ein, um Metadaten zu Personen zu extrahieren,
# wie Geburts- und Todesjahr, Kategorie (Stand/Tätigkeit) und Konfession.
# Dabei werden bestimmte Personengruppen (z.B. biblische Figuren) ausgeschlossen,
# denn auch sie können als 'present' markiert sein.

reg_file = 'register.xhtml'

try:
    tree_xhtml = ET.parse(reg_file)
except FileNotFoundError:
    print(f"Die Datei '{report_name}' wurde nicht gefunden.")
    sys.exit(1)
except ET.XMLSyntaxError as e:
    print(f"Fehler beim Parsen der XML-Datei '{report_name}':\n{e}")
    sys.exit(1)
except Exception as e:
    print(f"Ein unerwarteter Fehler ist aufgetreten:\n{e}")
    sys.exit(1)


# Liste zur Speicherung der angereicherten Personeninformationen aus dem Register
persons_reg = []

# Findet alle Personen-Einträge im Register basierend auf dem definierten XPath
entries_reg = tree_xhtml.xpath(xpath_entries)

# Tupel für Geburts- und Todesjahr aus dem Register
lifedata_reg = (extract_birth_date, extract_death_date)
    
# Ausschlusskategorien
excluded_category_keywords = ['Biblische Personen, Heilige', 'Mythologische Personen', 'Personifikationen'] # Ausschluss irrelevanter Kategorien im Register
excluded_name_keywords = ['biblisch', 'mythologisch', 'Theaterfigur', 'Personifikation', 'Apostel', 'Prophet', 'Jesus Christus'] # Ausschluss von Personentypen ohne konsistente Kategorieangaben

# Hauptschleife zur Zuweisung von Metadaten aus dem Register
for entry in entries_reg:
    # Name
    name = extract_xpath_text(entry, xpath_name)
    # Lebensdaten
    lifedata_reg = extract_xpath_text(entry, xpath_lifedata)
    # Personen ohne Lebensdaten werden nicht ausgeschlossen, da es sich oft um Personen niedrigeren sozialen Ranges handelt, die einen tiefen Quellenabdruck aufweisen. Für den Auschluss irrelevanter Einträge wie mythologischer Personen müssen Ausschlusskriterien herangezogen werden.
    # Schliesst Figuren und historische Persöhnlichkeiten aus, auch wenn diese unter dem Tag 'present' aufgeführt sind
    if any(cat in category for cat in excluded_category_keywords):
        continue
    # Schliesst Figuren und historische Persönlichkeiten aus, auch wenn sie nicht mit einer entsprechenden Kategorie im Register versehen sind
    if any(x in name.lower() for x in excluded_name_keywords):
        continue
    # Kategorie (Stand/Tätigkeit)
    category = extract_list_text(entry, xpath_category)
    # Konfession
    faith_reg = extract_list_text(entry, xpath_faith)
    # Folgt der Praxis des Registers: Wenn keine Konfession angegeben ist, wird 'keine Angabe' verwendet
    # Dies ist wichtig, um die Konsistenz mit der Edition zu wahren
    if faith_reg is None:
        faith_reg = 'keine Angabe'
    # Mehrere Konfessionen werden durch ein Semikolon getrennt und weisen auf eine Konversion hin
    # (so bedeutet z.B. '(katholisch; protestantisch)', dass die Person zuerst katholischen Glaubens war und dann zum Protestantismus konvertierte)

    # Fügt die Personendaten der Liste hinzu
    persons_reg.append({
        'Name': name,
        'Lebensdaten_Register': lifedata_reg,
        'Konfession_Register': faith_reg,
        'Kategorie': category
    })

# Erstellt einen DataFrame aus den gesammelten Register-Metadaten
df_reg = pd.DataFrame(persons_reg)

# Wendet die Bereinigungsfunktion auf alle Spalten des Metadaten-DataFrames an
for col in df_reg.columns:
    df_reg[col] = df_reg[col].apply(clean_cell)

# Gibt die Anzahl der erfassten Personen im Register aus
print(f"Register-Metadaten geladen. Es wurden {len(persons_reg)} Personen im Register erfasst.")


# --- Schritt 5: Prüfung auf Mehrdeutigkeiten ---
# Da die Metadaten über den Personennamen zugeordnet werden, kann es bei Namensvetter*innen zu falschen Datenzuweisungen kommen.
# Dies kommt im Personenregister tatsächlich vor (z.B. Magdalena von Bayern: (1388-1410) und (1587-1628)).
# Im Register-Dataframe werden in solchen Fällen die Metadaten der ersten Person erfasst.
# Da Metadaten aus dem Personenregister nicht mit den eindeutigen PSN-IDs verknüpft sind, muss die LOD-Datei bemüht werden.
# In der Linked Open Data sind einzelne die Personen und Orte betreffende Angaben enthalten.
# Ihre Bedeutungen können auf https://eos.hypotheses.org/293 nachgelesen werden.
# Bei Personen sind vor allem Lebensdaten von Bedeutung. Sind diese vorhanden, können sie als Unterscheidungsmerkmale dienen.
# Struktur df_reg: PSN_ID (eindeutig) -> Name (evtl. mehrdeutig) -> Lebensdaten_reg (fehleranfällig)
# Struktur df_lod:      PSN_ID (eindeutig) -> Lebensdaten_LOD (seltener vorhanden, jedoch korrekt zugewiesen)

df_lod_data = df_mapping.merge(df_lod, on='PSN_ID', how='left')
df_main = df_lod_data.merge(df_reg, on='Name', how='left')


df_main[['Lebensdaten', 'Konfession']] = df_main.apply(adjust_metadata, axis=1)
# Entfernt die ursprünglichen Lebensdaten-Spalten, da sie nun redundant sind
df_main.drop(columns=['Lebensdaten_LOD', 'Lebensdaten_Register'], inplace=True)

# Ausgabe des Ergebnisses
print(df_main)

# --- Schritt 6: Personen-DataFrames zusammenführen ---
# Hier werden die Personennennungen aus dem Bericht (df_mapping) mit den Metadaten aus dem Register (df_reg) zusammengeführt.
# Das Ergebnis ist ein umfassender DataFrame, der relevante Informationen zu den im Bericht genannten Personen enthält.

# Stellt sicher, dass die 'Name'-Spalten vor dem Mergen bereinigt sind
df_mapping['Name'] = df_mapping['Name'].str.strip()
df_reg['Name'] = df_reg['Name'].str.strip()

# Führt die beiden DataFrames basierend auf dem Personennamen zusammen (Left Join)
merged_df = pd.merge(df_mapping, df_reg, on='Name', how='left').fillna('').astype(str)

# Definiert die gewünschte Spaltenreihenfolge neu
# Extrahiert alle Spalten ausser 'Nennung'
cols = [col for col in merged_df.columns if col != 'Nennung']
# Fügt die breite Spalte 'Nennung' rechts hinzu
cols.append('Nennung')
# Erstellt einen DataFrame mit der neuen Spaltenreihenfolge
merged_df = merged_df[cols]

# Gibt den DataFrame in der Konsole aus
print("DataFrame mit Personendaten:")
print(merged_df)
# Gibt die Anzahl der Personen im zusammengeführten DataFrame aus
print(f"Zusammengeführter DataFrame enthält {len(merged_df)} Einträge.")

# --- Schritt 6: Ergebnisse speichern & Excel formatieren ---
# Dieser letzte Schritt speichert den zusammengeführten DataFrame
# sowohl als CSV- als auch als Excel-Datei.
# Die Excel-Datei wird zusätzlich formatiert: Spaltenbreiten werden angepasst
# und die Kopfzeile wird hervorgehoben, um die Lesbarkeit zu verbessern.

# Bereinigt den Dateinamen für die Ausgabe (ersetzt Umlaute, Leerzeichen etc.)
new_filename = report_name.lower().replace('ä', 'ae').replace('ö', 'oe').replace('ü', 'ue').replace('ß', 'ss').replace(' ', '_')

# Definiert die Ausgabedateien für CSV und Excel
output_csv = new_filename.replace('.xml', '_personen_metadaten.csv')
output_xlsx = new_filename.replace('.xml', '_personen_metadaten.xlsx')

# Speichert den DataFrame als CSV-Datei (UTF-8 mit BOM für bessere Kompatibilität)
merged_df.to_csv(output_csv, sep=';', index=False, encoding='utf-8-sig')
# Speichert den DataFrame als Excel-Datei
merged_df.to_excel(output_xlsx, index=False)

# Excel-Datei: Anpassung der Spaltenbreite und Styling für bessere Lesbarkeit
wb = load_workbook(output_xlsx)
ws = wb.active # Aktiviert das erste Arbeitsblatt

# Styling-Definitionen für Kopfzeile
header_font = Font(bold=True) # Fettgedruckte Schrift
header_fill = PatternFill("solid", fgColor="DDDDDD") # Hellgrauer Hintergrund

# Geht jede Spalte durch, um die Breite anzupassen und Kopfzeile zu formatieren
for col_num, column_cells in enumerate(ws.columns, 1):
    max_length = 0
    for cell in column_cells:
        # Formatiert die Kopfzeile (erste Zeile)
        if cell.row == 1:
            cell.font = header_font
            cell.fill = header_fill
        try:
            # Ermittelt die maximale Länge des Zellinhalts in der Spalte
            if cell.value:
                max_length = max(max_length, len(str(cell.value)))
        except:
            # Ignoriert Fehler beim Zugriff auf Zellwerte
            pass
    # Setzt die Spaltenbreite basierend auf der maximalen Länge, plus etwas Puffer
    ws.column_dimensions[get_column_letter(col_num)].width = max_length + 1

wb.save(output_xlsx)
